{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulator: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Random Forest (RF)](https://builtin.com/data-science/random-forest-algorithm) regressor is an example of an ensemble learning method. During training, multiple decision trees are generated. Each tree is trained on a different subset of the data. Each tree overfits on the different subsamples of data and features. However, by averaging over all the different trees, the overall variance of the forest is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index<a name=\"index\"></a>\n",
    "1. [Import packages](#imports)\n",
    "2. [Load data](#loadData)\n",
    "    1. [Load train data](#loadTrainData)\n",
    "    2. [Load test data](#loadTestData)\n",
    "3. [Emulator method](#emulator)\n",
    "    1. [Scale data](#scaleData)\n",
    "    2. [Train emulator](#trainEmu)\n",
    "    3. [Predict on test data](#predEmu)\n",
    "    4. [Plot results](#plotEmu)\n",
    "\n",
    "## 1. Import packages<a name=\"imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import pylab\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "matplotlib.rcParams['figure.dpi'] = 80\n",
    "textsize = 'x-large'\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (5, 4),\n",
    "         'axes.labelsize': textsize,\n",
    "         'axes.titlesize': textsize,\n",
    "         'xtick.labelsize': textsize,\n",
    "         'ytick.labelsize': textsize}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data<a name=\"loadData\"></a>\n",
    "\n",
    "### 2.1. Load train data<a name=\"loadTrainData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set here is the correlation function of galaxy clustering, for different cosmological model. Here we show a 3-free-parameter model; you can also play with a simpler 1-free-parameter model by commenting in the '1d' data (be sure to do this with the test set too). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '../data/cosmology_train_big.pickle'\n",
    "#path_train = '../data/cosmology_train.pickle'\n",
    "#path_train = '../data/cosmology_train_1d.pickle'\n",
    "with open(path_train, 'rb') as input_file:\n",
    "    data_train = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = data_train['input_data']\n",
    "number_train = input_train.shape[0]\n",
    "print(\"Number of datapoints:\", number_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train = data_train['output_data']\n",
    "n_params = input_train.shape[1]-1\n",
    "n_values = output_train.shape[1]-1\n",
    "print(\"Number of input parameters:\", n_params)  # remove the `object_id` column\n",
    "print(\"Number of output values:\", n_values)  # remove the `object_id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train = np.array(input_train.drop(columns=['object_id']))\n",
    "ys_train = np.array(output_train.drop(columns=['object_id']))\n",
    "extra_train = data_train['extra_input']\n",
    "r_vals = extra_train['r_vals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Load test data<a name=\"loadTestData\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = '../data/cosmology_test.pickle'\n",
    "#path_test = '../data/cosmology_test_1d.pickle'\n",
    "with open(path_test, 'rb') as input:\n",
    "    data_test = pickle.load(input)\n",
    "input_test = data_test['input_data']\n",
    "number_test = input_test.shape[0]\n",
    "print(\"Number of datapoints:\", number_test)\n",
    "output_test = data_test['output_data']\n",
    "print(\"Number of input parameters:\", input_test.shape[1]-1)  # remove the `object_id` column\n",
    "print(\"Number of output values:\", output_test.shape[1]-1)  # remove the `object_id` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_test = np.array(input_test.drop(columns=['object_id']))\n",
    "ys_test = np.array(output_test.drop(columns=['object_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Emulator method<a name=\"emulator\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Scale data<a name=\"scaleData\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first scale our input parameters, to make training easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(xs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_train = scaler.transform(xs_train)\n",
    "xs_test = scaler.transform(xs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also normalize the output data by the mean of the training data, so it's easier to emulate (don't forget to undo the normalization after!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean = np.mean(ys_train, axis=0)\n",
    "ys_train = ys_train/y_mean\n",
    "ys_test = ys_test/y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Train emulator<a name=\"trainEmu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `scikit-learn`'s `RandomForestRegressor` to build our emulator. We train a separate regressor for each output value. \n",
    "\n",
    "For this dataset, the simple 'lbfgs' solver works very well. We can also use the 'adam' optimizer; in both cases, we have to tune the hyperparameters carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrs = np.empty(n_values, dtype=object)\n",
    "for j in range(n_values):\n",
    "    ys_train_r = ys_train[:,j]\n",
    "    ys_test_r = ys_test[:,j]\n",
    "    regr = RandomForestRegressor(n_estimators=1000, n_jobs=-1).fit(xs_train, ys_train_r)\n",
    "    score = regr.score(xs_test, ys_test_r)\n",
    "    print(f\"Value {j} score:\", score)\n",
    "    regrs[j] = regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values should be as close to 1 as possible. Try tuning the hyperparameter `n_estimators` to get better results on your test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Predict on test data<a name=\"predEmu\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can predict on our test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_predict = np.zeros((number_test, n_values))\n",
    "for j in range(n_values):  \n",
    "    ys_predict_r = regrs[j].predict(xs_test)\n",
    "    ys_predict[:,j] = ys_predict_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undo all the normalizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train = ys_train*y_mean\n",
    "ys_test = ys_test*y_mean\n",
    "ys_predict = ys_predict*y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Plot results<a name=\"plotEmu\"></a>\n",
    "\n",
    "We compare our predictions to the truth (choosing a subset for visual clarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plot = int(0.2*number_test)\n",
    "idxs = np.random.choice(np.arange(number_test), n_plot)\n",
    "color_idx = np.linspace(0, 1, n_plot)\n",
    "colors = np.array([plt.cm.rainbow(c) for c in color_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "for i in range(n_plot):\n",
    "    ys_test_plot = ys_test[idxs,:][i]\n",
    "    ys_predict_plot = ys_predict[idxs][i]\n",
    "    if i==0:\n",
    "        label_test = 'truth'\n",
    "        label_predict = 'emu_prediction'\n",
    "    else:\n",
    "        label_test = None\n",
    "        label_predict = None\n",
    "    plt.plot(r_vals[:n_values], ys_test_plot, alpha=0.8, label=label_test, marker='o', markerfacecolor='None', ls='None', color=colors[i])\n",
    "    plt.plot(r_vals[:n_values], ys_predict_plot, alpha=0.8, label=label_predict, color=colors[i])\n",
    "plt.xlabel('$r$')\n",
    "plt.ylabel(r'$\\xi(r)$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the fractional error of all test set statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_idx = np.linspace(0, 1, number_test)\n",
    "colors = np.array([plt.cm.rainbow(c) for c in color_idx])\n",
    "plt.figure(figsize=(8,6))\n",
    "frac_errs = np.empty((number_test, n_values))\n",
    "for i in range(number_test):\n",
    "#     ys_test_plot = ys_test[idxs,:][i]\n",
    "#     ys_predict_plot = ys_predict[idxs][i]\n",
    "    ys_test_plot = ys_test[i]\n",
    "    ys_predict_plot = ys_predict[i]\n",
    "    frac_err = (ys_predict_plot-ys_test_plot)/ys_test_plot\n",
    "    frac_errs[i] = frac_err\n",
    "    plt.plot(r_vals, frac_err, alpha=0.8, color=colors[i])\n",
    "plt.axhline(0.0, color='k')\n",
    "plt.xlabel('$r$')\n",
    "plt.ylabel(r'fractional error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The emulator is sort of working but it's not great! In particular it struggles significantly with one of the r-bins. This r-bin is likely more difficult to emulate because it contains large range of values and both positive and negative values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_idx = np.linspace(0, 1, number_test)\n",
    "colors = np.array([plt.cm.rainbow(c) for c in color_idx])\n",
    "plt.figure(figsize=(8,6))\n",
    "frac_errs_stdev = np.std(frac_errs, axis=0)\n",
    "plt.plot(r_vals, frac_errs_stdev, alpha=0.8, color='blue', label='standard deviation')\n",
    "\n",
    "frac_errs_p16 = np.percentile(frac_errs, 16, axis=0)\n",
    "frac_errs_p84 = np.percentile(frac_errs, 84, axis=0)\n",
    "frac_errs_percentile = np.mean([np.abs(frac_errs_p16), np.abs(frac_errs_p84)], axis=0)\n",
    "plt.plot(r_vals, frac_errs_percentile, alpha=0.8, color='green', label=\"mean of 16/84 percentile\")\n",
    "\n",
    "\n",
    "plt.xlabel('$r$')\n",
    "plt.ylabel(r'spread of fractional errors')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the spike at r=120 is caused by a few outliers, as our percentile statistic is more stable than the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to top.](#index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
